{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Dengue Case Severity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This AI model aims to predict dengue cases classification using different AI models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was obtained from https://www.kaggle.com/datasets/siddhvr/dengue-predictionfetal-health-classification from the author Siddhvr.<br><br>\n",
    "\n",
    "The dataset has the following features: \n",
    "\n",
    "**serial**: It represents a unique identifier for each data entry. It is likely used for tracking or indexing purposes.\n",
    "\n",
    "**tempmax**: It refers to the maximum temperature recorded for a specific time period.\n",
    "\n",
    "**tempmin**: It represents the minimum temperature recorded for a specific time period.\n",
    "\n",
    "**temp**: It denotes the average temperature during the specified time period.\n",
    "\n",
    "**feelslikemax**: It indicates the maximum \"feels like\" temperature, which takes into account factors such as humidity and wind to estimate how the temperature actually feels.\n",
    "\n",
    "**feelslikemin**: It represents the minimum \"feels like\" temperature during the specified time period.\n",
    "\n",
    "**feelslike**: It denotes the average \"feels like\" temperature, which is an estimation of how the temperature feels to humans.\n",
    "\n",
    "**dew**: It refers to the dew point, which is the temperature at which air becomes saturated with moisture, leading to the formation of dew.\n",
    "\n",
    "**humidity**: It represents the relative humidity, indicating the amount of moisture present in the air relative to the maximum amount it could hold at that temperature.\n",
    "\n",
    "**precip**: It denotes the total precipitation (rainfall) recorded during the specified time period.\n",
    "\n",
    "**precipprob**: It represents the probability of precipitation occurring during the specified time period.\n",
    "\n",
    "**precipcover**: It indicates the coverage or extent of precipitation in the given area during the specified time period.\n",
    "\n",
    "**snow**: It represents the amount of snowfall recorded during the specified time period.\n",
    "\n",
    "**snowdepth**: It denotes the depth of snow on the ground during the specified time period.\n",
    "\n",
    "**windspeed**: It represents the speed of wind recorded during the specified time period.\n",
    "\n",
    "**winddir**: It indicates the direction from which the wind is blowing during the specified time period.\n",
    "\n",
    "**sealevelpressure**: It refers to the atmospheric pressure at sea level during the specified time period.\n",
    "\n",
    "**cloudcover**: It represents the extent of cloud cover during the specified time period.\n",
    "\n",
    "**visibility**: It denotes the horizontal visibility, indicating how far an observer can see clearly during the specified time period.\n",
    "\n",
    "**solarradiation**: It represents the amount of solar radiation received during the specified time period.\n",
    "\n",
    "**solarenergy**: It indicates the solar energy level during the specified time period.\n",
    "\n",
    "**uvindex**: It represents the UV index, which is a measure of the intensity of ultraviolet (UV) radiation from the sun.\n",
    "\n",
    "**conditions**: It represents the weather conditions during the specified time period, such as sunny, cloudy, rainy, etc.\n",
    "\n",
    "**stations**: It refers to the number of weather stations used to collect the data for the given region.\n",
    "\n",
    "**cases**: It denotes the number of dengue cases recorded in the specified region during the specified time period.\n",
    "\n",
    "**labels**: It represents the severity of dengue cases.\n",
    "<br><br>\n",
    "The model will try to predict the severity level of dengue cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary dependencies\n",
    "\n",
    "```\n",
    "pip install xgboost catboost pandas scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_csv('dengue-dataset.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle missing values\n",
    "\n",
    "Identified missing values in features. If there are empty values, the data containing the empty value is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty values in the DataFrame\n",
    "print(data.isnull().any())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling\n",
    "\n",
    "Normalize numerical features with RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns to exclude from normalization\n",
    "target_column = 'labels'\n",
    "columns_to_exclude = ['serial', 'cases', target_column]\n",
    "\n",
    "# Create a DataFrame with the columns to normalize\n",
    "data_to_normalize = data.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Create an instance of StandardScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Normalize the data\n",
    "normalized_data = scaler.fit_transform(data_to_normalize)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=data_to_normalize.columns)\n",
    "\n",
    "# Combine the excluded columns with the normalized DataFrame\n",
    "data = pd.concat([data[columns_to_exclude], normalized_df], axis=1)\n",
    "\n",
    "# Print the final data\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical encoding\n",
    "\n",
    "Values in the **labels** feature is mapped with these values:\n",
    "\n",
    "**Severe Risk**: 4,\n",
    "**High Risk**: 3,\n",
    "**Moderate Risk**: 2,\n",
    "**Low Risk**: 1,\n",
    "**Minimal to No risk**: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label-value mapping\n",
    "desired_order = ['Minimal to No risk', 'Low Risk', 'Moderate Risk', 'High Risk', 'Severe Risk']\n",
    "label_mapping = {\n",
    "    'Severe Risk': 4,\n",
    "    'High Risk': 3,\n",
    "    'Moderate Risk': 2,\n",
    "    'Low Risk': 1,\n",
    "    'Minimal to No risk': 0\n",
    "}\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit LabelEncoder with the unique labels from mapping dictionary\n",
    "label_encoder.fit(desired_order)\n",
    "\n",
    "# Encode the labels using the mapping dictionary\n",
    "data['labels'] = data['labels'].map(label_mapping)\n",
    "\n",
    "data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Dataset\n",
    "\n",
    "The training set was used to train each model, while the testing set was used for validation (Train set: 76%, Test set: 24%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns to exclude from the train set\n",
    "target_column = 'labels'\n",
    "columns_to_exclude = ['serial', 'cases',target_column]\n",
    "\n",
    "# Assign X to train variable and y to target variable\n",
    "X = data.drop(columns=columns_to_exclude)\n",
    "y = data[target_column]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.24, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models and fit it to the training data\n",
    "XGBmodel = xgb.XGBClassifier()\n",
    "SVMmodel = svm.SVC()\n",
    "CBmodel = CatBoostClassifier()\n",
    "RFmodel = RandomForestClassifier()\n",
    "MLPmodel = MLPClassifier()\n",
    "\n",
    "RFmodel.fit(X_train, y_train)\n",
    "XGBmodel.fit(X_train, y_train)\n",
    "SVMmodel.fit(X_train, y_train)\n",
    "CBmodel.fit(X_train, y_train)\n",
    "MLPmodel.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on the test sets and compare errors and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "cb_y_pred = CBmodel.predict(X_test)\n",
    "cb_mse = mean_squared_error(y_test, cb_y_pred)\n",
    "cb_accuracy = r2_score(y_test, cb_y_pred)\n",
    "print('Catboost Mean Squared Error:', cb_mse)\n",
    "print(\"Catboost RMSE\",np.sqrt(mean_squared_error(y_test,cb_y_pred)))\n",
    "print('Catboost MAE:', mean_absolute_error(y_test,cb_y_pred))\n",
    "print('Catboost R2 Accuracy: ', cb_accuracy)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_y_pred = RFmodel.predict(X_test)\n",
    "rf_mse = mean_squared_error(y_test, rf_y_pred)\n",
    "rf_accuracy = r2_score(y_test, rf_y_pred)\n",
    "print('RF Mean Squared Error:', rf_mse)\n",
    "print(\"RF RMSE\",np.sqrt(mean_squared_error(y_test,rf_y_pred)))\n",
    "print('RF MAE:', mean_absolute_error(y_test,rf_y_pred))\n",
    "print('RF R2 Accuracy: ', rf_accuracy)\n",
    "\n",
    "# XGBoost\n",
    "xgb_y_pred = XGBmodel.predict(X_test)\n",
    "xgb_mse = mean_squared_error(y_test, xgb_y_pred)\n",
    "xgb_accuracy = r2_score(y_test, xgb_y_pred)\n",
    "print('XGB Mean Squared Error:', xgb_mse)\n",
    "print(\"XGB RMSE\",np.sqrt(mean_squared_error(y_test,xgb_y_pred)))\n",
    "print('XGB MAE:', mean_absolute_error(y_test,xgb_y_pred))\n",
    "print('XGB R2 Accuracy: ', xgb_accuracy)\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_y_pred = SVMmodel.predict(X_test)\n",
    "svm_mse = mean_squared_error(y_test, svm_y_pred)\n",
    "svm_accuracy = r2_score(y_test, svm_y_pred)\n",
    "print('SVM Mean Squared Error:', svm_mse)\n",
    "print(\"SVM RMSE\",np.sqrt(mean_squared_error(y_test,svm_y_pred)))\n",
    "print('SVM MAE:', mean_absolute_error(y_test,svm_y_pred))\n",
    "print('SVM R2 Accuracy: ', svm_accuracy)\n",
    "\n",
    "# Multilayer Perceptrion\n",
    "mlp_y_pred = MLPmodel.predict(X_test)\n",
    "mlp_mse = mean_squared_error(y_test, mlp_y_pred)\n",
    "mlp_accuracy = r2_score(y_test, mlp_y_pred)\n",
    "print('MLP Mean Squared Error:', mlp_mse)\n",
    "print(\"MLP RMSE\",np.sqrt(mean_squared_error(y_test,mlp_y_pred)))\n",
    "print('MLP MAE:', mean_absolute_error(y_test,mlp_y_pred))\n",
    "print('MLP R2 Accuracy: ', mlp_accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the model by hyperparameter tuning\n",
    "\n",
    "To further improve the R2 accuracy score and errors, GridSearchCV would be used to find the best parameters for each models. Irrelevant features will be removed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns to exclude from the train set\n",
    "target_column = 'labels'\n",
    "columns_to_exclude = ['serial','cases', target_column]\n",
    "\n",
    "# Assign X to train variable and y to target variable\n",
    "X = data.drop(columns=columns_to_exclude)\n",
    "y = data[target_column]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.24, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFmodel = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid for tuning:\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],        # Number of trees\n",
    "    'max_depth': [None, 5, 10],              # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],         # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],           # Minimum number of samples required to be at a leaf node\n",
    "    'bootstrap': [True, False]               # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best parameters:\n",
    "grid_search = GridSearchCV(estimator=RFmodel, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score:\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "\n",
    "# Get the best model with the tuned parameters:\n",
    "rf_best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model:\n",
    "rf_y_pred = rf_best_model.predict(X_test)\n",
    "rf_mse = mean_squared_error(y_test, rf_y_pred)\n",
    "rf_accuracy = r2_score(y_test, rf_y_pred)\n",
    "print('RF Mean Squared Error:', rf_mse)\n",
    "print(\"RF RMSE\",np.sqrt(mean_squared_error(y_test,rf_y_pred)))\n",
    "print('RF MAE:', mean_absolute_error(y_test,rf_y_pred))\n",
    "print('RF R2 Accuracy: ', rf_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBmodel = xgb.XGBClassifier()\n",
    "\n",
    "# Define the parameter grid for tuning:\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of boosting rounds\n",
    "    'max_depth': [3, 5, 7],           # Maximum depth of each tree\n",
    "    'learning_rate': [0.1, 0.01, 0.001],  # Learning rate\n",
    "    'subsample': [0.8, 1.0],          # Subsample ratio of the training instances\n",
    "    'colsample_bytree': [0.8, 1.0]    # Subsample ratio of columns when constructing each tree\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best parameters:\n",
    "grid_search = GridSearchCV(estimator=XGBmodel, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score:\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "\n",
    "# Get the best model with the tuned parameters:\n",
    "xgb_best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model:\n",
    "xgb_y_pred = xgb_best_model.predict(X_test)\n",
    "xgb_mse = mean_squared_error(y_test, xgb_y_pred)\n",
    "xgb_accuracy = r2_score(y_test, xgb_y_pred)\n",
    "print('XGB Mean Squared Error:', xgb_mse)\n",
    "print(\"XGB RMSE\",np.sqrt(mean_squared_error(y_test,xgb_y_pred)))\n",
    "print('XGB MAE:', mean_absolute_error(y_test,xgb_y_pred))\n",
    "print('XGB R2 Accuracy: ', xgb_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMmodel = svm.SVC()\n",
    "\n",
    "# Define the parameter grid for tuning:\n",
    "param_grid = {\n",
    "    'C': [0.1, 10, 100, 1000],               # Penalty parameter C of the error term\n",
    "    'kernel': ['linear', 'rbf'],     # Kernel type: linear or radial basis function (rbf)\n",
    "    'gamma': ['scale', 'auto'],      # Kernel coefficient for 'rbf': scale or auto\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best parameters:\n",
    "grid_search = GridSearchCV(estimator=SVMmodel, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score:\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "\n",
    "# Get the best model with the tuned parameters:\n",
    "svm_best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model:\n",
    "svm_y_pred = svm_best_model.predict(X_test)\n",
    "svm_mse = mean_squared_error(y_test, svm_y_pred)\n",
    "svm_accuracy = r2_score(y_test, svm_y_pred)\n",
    "print('SVM Mean Squared Error:', svm_mse)\n",
    "print(\"SVM RMSE\",np.sqrt(mean_squared_error(y_test,svm_y_pred)))\n",
    "print('XGB MAE:', mean_absolute_error(y_test,svm_y_pred))\n",
    "print('SVM R2 Accuracy: ', svm_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBmodel = CatBoostClassifier()\n",
    "\n",
    "# Define the parameter grid for tuning:\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01],   # Learning rate\n",
    "    'depth': [4, 6, 8],                    # Tree depth\n",
    "    'l2_leaf_reg': [1, 3, 5]               # L2 regularization\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best parameters:\n",
    "grid_search = GridSearchCV(estimator=CBmodel, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score:\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "\n",
    "# Get the best model with the tuned parameters:\n",
    "cb_best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model:\n",
    "cb_y_pred = cb_best_model.predict(X_test)\n",
    "cb_mse = mean_squared_error(y_test, cb_y_pred)\n",
    "cb_accuracy = r2_score(y_test, cb_y_pred)\n",
    "print('Catboost Mean Squared Error:', cb_mse)\n",
    "print(\"Catboost RMSE\",np.sqrt(mean_squared_error(y_test,cb_y_pred)))\n",
    "print('Catboost MAE:', mean_absolute_error(y_test,cb_y_pred))\n",
    "print('Catboost R2 Accuracy: ', cb_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPmodel = MLPClassifier()\n",
    "\n",
    "# Define the parameter grid for tuning:\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (50, 50), (100, 50)],   # Sizes of hidden layers\n",
    "    'activation': ['relu', 'tanh'],                        # Activation function\n",
    "    'solver': ['adam', 'sgd'],                              # Optimization algorithm\n",
    "    'alpha': [0.0001, 0.001, 0.01],                         # L2 penalty (regularization term)\n",
    "    'learning_rate': ['constant', 'adaptive']               # Learning rate schedule\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best parameters:\n",
    "grid_search = GridSearchCV(estimator=MLPmodel, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score:\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "\n",
    "# Get the best model with the tuned parameters:\n",
    "mlp_best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model:\n",
    "mlp_y_pred = mlp_best_model.predict(X_test)\n",
    "mlp_mse = mean_squared_error(y_test, mlp_y_pred)\n",
    "mlp_accuracy = r2_score(y_test, mlp_y_pred)\n",
    "print('MLP Mean Squared Error:', mlp_mse)\n",
    "print(\"MLP RMSE\",np.sqrt(mean_squared_error(y_test,mlp_y_pred)))\n",
    "print('MLP MAE:', mean_absolute_error(y_test,mlp_y_pred))\n",
    "print('MLP R2 Accuracy: ', mlp_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
